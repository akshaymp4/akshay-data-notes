{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"AWS/aws_notes/","title":"Amazon Web Service","text":""},{"location":"AWS/aws_notes/#welcome-to-cloud-for-data-scineceanalyticsengineering","title":"Welcome to Cloud for Data Scinece/Analytics/Engineering","text":"<p>Amazon Web Services (AWS) for Data Science</p> <p>1\ufe0f\u20e3 What is AWS? Definition</p> <p>Amazon Web Services (AWS) is a cloud computing platform by Amazon that provides scalable infrastructure and services over the internet.</p> <p>Instead of buying physical servers, you can: Rent virtual machines Store data Deploy applications Build ML models All on-demand.</p> <p>2\ufe0f\u20e3 Why AWS for Data Science?</p> <p>Traditional Setup Problem : </p> <p>Buy servers</p> <p>Configure networking</p> <p>Maintain hardware</p> <p>Scale manually</p> <p>High upfront cost</p> <p>AWS Solution</p> <p>No hardware Instant server creation</p> <p>Auto scaling</p> <p>Pay only for usage</p> <p>Managed ML services</p> <p>3\ufe0f\u20e3 Core AWS Concepts</p> <p>\u2601\ufe0f 3.1 Cloud Computing Definition:</p> <p>Delivering computing services over the internet.</p> <p>Types:</p> <p>IaaS \u2013 Infrastructure as a Service (EC2) PaaS \u2013 Platform as a Service (Elastic Beanstalk) SaaS \u2013 Software as a Service (Gmail, Zoom)</p> <p>\ud83c\udf0d 3.2 Regions &amp; Availability Zones</p> <p>A Region is a geographic area where AWS has data centers. Example: Mumbai (ap-south-1), US-East (Virginia).</p> <p>Each Region contains multiple Availability Zones (AZs).</p> <p>Why this matters:</p> <p>If one data center fails \u2192 other AZ works.</p> <p>Used for high availability.</p> <p>Critical for production systems.</p> <p>4\ufe0f\u20e3 AWS Storage Services</p> <p>4.1 Amazon S3 (Simple Storage Service)</p> <p> </p> <p>Definition:</p> <p>Object storage service used to store files.</p> <p>Key Concepts: Bucket \u2192 Folder Object \u2192 File Unlimited storage Highly durable (99.999999999%)</p> <p>Use Cases:</p> <p>Store CSV datasets Store ML models Store backups Host static websites</p> <p>\ud83d\udcbe 4.2 EBS (Elastic Block Store) Definition:</p> <p>Block storage attached to EC2. Works like Hard Disk Used for OS, database storage Persistent storage</p> <p>\ud83e\uddca 4.3 Glacier Definition:</p> <p>Low-cost storage for archives. Cheap Slow retrieval Used for backups</p> <p>5\ufe0f\u20e3 AWS Compute Services \ud83d\udda5 5.1 EC2 (Elastic Compute Cloud)</p> <p> </p> <p>Definition:</p> <p>EC2 instances come in different types:</p> <p>General Purpose \u2192 t2, t3 (small apps, freelancing)</p> <p>Compute Optimized \u2192 c5 (ML training)</p> <p>Memory Optimized \u2192 r5 (big datasets)</p> <p>GPU \u2192 p3, g4 (Deep learning)</p> <p>You choose instance type based on:</p> <p>RAM requirement</p> <p>CPU cores</p> <p>GPU need</p> <p>Budget</p> <p>You can:</p> <p>Install Python Install MySQL Run ML models Host applications</p> <p>Key Components:</p> <p>AMI (Amazon Machine Image) \u2192 OS template Instance Type \u2192 CPU/RAM configuration Key Pair \u2192 SSH access Security Group \u2192 Firewall</p> <p>EC2 for Data Science:</p> <p>Create Ubuntu server Install Anaconda Connect via SSH Run Jupyter Notebook</p> <p>\u26a1 5.2 AWS Lambda Definition:</p> <p>Serverless compute service.</p> <p>No server management Event-driven Pay per execution</p> <p>Used for:</p> <p>Triggering pipeline jobs Automating workflows</p> <p>6\ufe0f\u20e3 Networking Basics</p> <p>\ud83c\udf10 6.1 VPC (Virtual Private Cloud)</p> <p>Definition:</p> <p>Create public servers (accessible from internet)</p> <p>Create private servers (internal only)</p> <p>Control IP ranges</p> <p>Secure databases</p> <p>Example:</p> <p>EC2 in public subnet</p> <p>RDS in private subnet</p> <p>You control:</p> <p>IP ranges</p> <p>Subnets</p> <p>Routing</p> <p>Security</p> <p>\ud83d\udd10 Security Group</p> <p>Acts as:</p> <p>Firewall for EC2</p> <p>Controls:</p> <p>Inbound traffic Outbound traffic</p> <p>Example: Allow port 22 for SSH Allow port 8888 for Jupyter</p> <p>7\ufe0f\u20e3 AWS CLI Definition:</p> <p>Command Line Interface to control AWS from terminal.</p> <p>Example: aws s3 ls aws ec2 describe-instances</p> <p>Used for:</p> <p>Automation</p> <p>DevOps</p> <p>CI/CD</p> <p>8\ufe0f\u20e3 Application Deployment in AWS Deployment Methods: 1\ufe0f\u20e3 Manual EC2 Deployment</p> <p>Launch EC2</p> <p>Install software Upload code Run app</p> <p>2\ufe0f\u20e3 Elastic Beanstalk</p> <p>Upload code AWS manages infrastructure</p> <p>3\ufe0f\u20e3 Docker + EC2</p> <p>Container-based deployment</p> <p>9\ufe0f\u20e3 AWS SageMaker (For Machine Learning)</p> <p> </p> <p>Definition:</p> <p>SageMaker is a fully managed ML platform.</p> <p>It removes the need to:</p> <p>Launch EC2 manually</p> <p>Configure GPUs</p> <p>Install ML libraries</p> <p>It provides:</p> <p>Notebook environment</p> <p>Built-in algorithms</p> <p>Distributed training</p> <p>One-click deployment</p> <p>Model monitoring</p> <p>You can:</p> <p>Build models Train models Deploy models Monitor models</p> <p>SageMaker Workflow</p> <p>Upload data to S3 Create Notebook instance Train model Deploy endpoint</p> <p>Use API for predictions</p> <p>Why SageMaker?</p> <p>No server setup</p> <p>Auto scaling</p> <p>Built-in algorithms</p> <p>Production ready</p> <p>\ud83d\udd1f IAM (Identity and Access Management) Definition:</p> <p>IAM controls access inside AWS.</p> <p>Components:</p> <p>User \u2192 Person</p> <p>Role \u2192 Permission set assigned to service</p> <p>Policy \u2192 JSON document defining allowed actions</p> <p>Example:</p> <p>Give EC2 permission to read S3</p> <p>Restrict intern from deleting resources</p> <p>You can:</p> <p>Create users Assign roles Attach policies</p> <p>Important for:</p> <p>Security Controlled access</p> <p>1\ufe0f\u20e31\ufe0f\u20e3 Databases in AWS RDS (Relational Database Service) Definition:</p> <p>Managed SQL database.</p> <p>Supports:</p> <p>MySQL PostgreSQL MariaDB</p> <p>No need to manage:</p> <p>Backups Scaling Patching</p> <p>1\ufe0f\u20e32\ufe0f\u20e3 Data Science Architecture (Beginner Level)</p> <p>Example Workflow:</p> <p>Store raw data \u2192 S3 Launch EC2 Connect to S3 Train model Save model in S3</p> <p>Deploy via EC2 or SageMaker</p> <p>1\ufe0f\u20e33\ufe0f\u20e3 Pay-As-You-Go Model</p> <p>AWS charges based on:</p> <p>Compute hours Storage used Data transfer API calls</p> <p>No upfront cost.</p> <p>1\ufe0f\u20e34\ufe0f\u20e3 When to Use What? Requirement Service Store datasets  S3 Run Python code EC2 Serverless automation   Lambda Train ML models easily  SageMaker SQL database    RDS Archive data    Glacier</p> <p>1\ufe0f\u20e35\ufe0f\u20e3 Advantages of AWS</p> <p>Scalable Reliable Secure Global infrastructure Large ecosystem Free Tier available</p> <p>1\ufe0f\u20e36\ufe0f\u20e3 AWS for Freelancers (Your Use Case)</p> <p>Since you:</p> <p>Work with small data Do freelance analytics Use MySQL + Python</p> <p>Best setup:</p> <p>Store data \u2192 S3 Compute \u2192 Small EC2 (t2.micro / t3.micro) Database \u2192 RDS MySQL Deployment \u2192 EC2 or Elastic Beanstalk</p> <p>Cost can be very low if optimized.</p> <p>\ud83d\ude80 Final Summary</p> <p>AWS provides:</p> <p>Storage (S3) Compute (EC2, Lambda) ML Platform (SageMaker) Databases (RDS) Networking (VPC) Security (IAM)</p> <p>It allows data scientists to:</p> <p>Build Train Deploy Scale</p> <p>Without managing physical infrastructure.</p> <p>\u2705 PART 2 \u2014 AWS Products with Normal Equivalent</p> <p>This is what you asked clearly \ud83d\udc4d</p>"},{"location":"AWS/aws_notes/#aws-products-and-their-normal-equivalent","title":"AWS Products and Their Normal Equivalent","text":"AWS Product Category What It Is Normal Equivalent (Traditional Setup) EC2 Compute Virtual machine in cloud Physical server / Your laptop S3 Object Storage Stores files (objects) Google Drive / External storage EBS Block Storage Hard disk attached to EC2 Internal HDD / SSD RDS Relational Database Managed SQL database MySQL installed on server DynamoDB NoSQL Database Managed NoSQL database MongoDB Redshift Data Warehouse Analytical database for BI &amp; reporting Snowflake / On-prem Data Warehouse Athena Query Engine Query S3 using SQL Presto / Querying CSV locally Glue ETL Service Data pipeline &amp; transformation service Talend / Manual Python ETL scripts SageMaker ML Platform Build, train &amp; deploy ML models Jupyter + Flask + Manual deployment Lambda Serverless Compute Run code without managing servers Cron job / Background script Elastic Beanstalk PaaS Easy application deployment Heroku VPC Networking Private network inside AWS Office LAN network"},{"location":"AWS/aws_notes/#database-vs-data-warehouse-vs-data-lake","title":"Database vs Data Warehouse vs Data Lake","text":"Feature Database (OLTP) Data Warehouse (OLAP) Data Lake Main Purpose Daily transactions Business analytics Store raw data Data Type Structured Structured Structured + Unstructured Example AWS RDS / DynamoDB Redshift S3 Query Type Simple queries Complex analytical queries Process after storing Schema Schema-on-write Schema-on-write Schema-on-read Data Volume Medium Large Very Large Users Application systems Analysts / BI tools Data Engineers / Scientists Cost Moderate Higher Cheapest storage"},{"location":"DataBricks/introduction/","title":"1\ufe0f\u20e3 Introduction","text":"<p>\ud83d\udcd8 What is Databricks?</p> <p>Databricks is a cloud-based unified data and AI platform built on top of Apache Spark that enables organizations to process, analyze, and build machine learning solutions on large-scale data.</p> <p>It is designed around the Lakehouse architecture, which combines:</p> <p>Data Lake capabilities \u2192 Low-cost storage, flexibility, support for structured and unstructured data</p> <p>Data Warehouse capabilities \u2192 High performance, ACID transactions, governance, and BI optimization</p> <p>This means Databricks gives you the flexibility of a data lake and the reliability of a data warehouse in a single platform.</p> <p>\ud83d\ude80 Why Databricks is Called a Managed Service</p> <p>Databricks is a managed service, meaning you do not need to manually set up or maintain infrastructure.</p> <p>Instead of configuring servers, installing Spark, handling failures, and tuning performance \u2014 Databricks manages these for you automatically.</p> <p>It takes care of:</p> <p>Cluster management Infrastructure provisioning Scaling Performance optimization Security integrations</p> <p>This allows:</p> <p>Data Engineers \u2192 to focus on pipelines</p> <p>Analysts \u2192 to focus on insights</p> <p>Data Scientists \u2192 to focus on models</p> <p>Instead of spending time on DevOps or infrastructure setup.</p> <p>1\ufe0f\u20e3 Cluster Management</p> <p>Cluster management refers to the automatic creation, configuration, monitoring, and termination of compute clusters used to process data.</p> <p>\ud83d\udc49 Databricks automatically manages Spark clusters so users don\u2019t need to manually configure servers.</p> <p>2\ufe0f\u20e3 Infrastructure Provisioning</p> <p>Infrastructure provisioning is the process of setting up cloud resources such as virtual machines, storage, and networking.</p> <p>\ud83d\udc49 Databricks automatically provisions the required cloud infrastructure (AWS, Azure, GCP) when you start a cluster.</p> <p>3\ufe0f\u20e3 Scaling</p> <p>Scaling is the ability to increase or decrease computing resources based on workload demand.</p> <p>\ud83d\udc49 Databricks supports auto-scaling, meaning it can add or remove worker nodes automatically depending on workload size.</p> <p>4\ufe0f\u20e3 Performance Optimization</p> <p>Performance optimization involves tuning system resources and execution strategies to run workloads faster and more efficiently.</p> <p>\ud83d\udc49 Databricks optimizes Spark jobs automatically using features like query optimization, caching, and optimized execution engines.</p> <p>5\ufe0f\u20e3 Security Integrations</p> <p>Security integrations ensure that data access and system usage are secure and compliant with organizational policies.</p> <p>\ud83d\udc49 Databricks integrates with cloud IAM systems, role-based access control, encryption, and Unity Catalog for governance.</p> <p>\ud83d\udca1 In Simple Words </p> <p>Without Databricks \u2192 You manage servers, install Spark, scale manually, secure everything yourself.</p> <p>With Databricks \u2192 You just write code. The platform manages everything else</p> <p>This allows data engineers, analysts, and data scientists to focus on building data solutions instead of managing infrastructure.</p>"},{"location":"DataBricks/introduction/#lakehouse-architecture-workflow-diagram","title":"Lakehouse architecture workflow diagram","text":"<p>2\ufe0f\u20e3 Key Components of Databricks</p> <p>Workspaces \u2013 Collaborative environment for notebooks, jobs, and dashboards</p> <p>Clusters \u2013 Compute resources to run Spark workloads</p> <p>DBFS (Databricks File System) \u2013 Distributed file system abstraction</p> <p>Delta Lake \u2013 Storage layer providing ACID transactions</p> <p>Unity Catalog \u2013 Centralized governance layer</p> <p>Competetors of Delta lake are : </p> <pre><code>Apache Iceberg, \nApache Hudi, \nSnowflake, \nMicrosoft Fabric (OneLake)\n</code></pre>"},{"location":"DataBricks/introduction/#data-lake-vs-data-warehouse-vs-lakehouse","title":"\ud83d\udd0e Data Lake vs Data Warehouse vs Lakehouse","text":"Feature Data Lake Data Warehouse Lakehouse (Databricks) Storage Cost Low High Low Schema Flexible Structured Structured + Flexible Performance Medium High High ACID Support \u274c No \u2705 Yes \u2705 Yes (Delta Lake) Governance Limited Strong Strong (Unity Catalog) Supports ML \u2705 Yes Limited \u2705 Yes <p>3\ufe0f\u20e3 What is Metadata?</p> <p>Metadata is \u201cdata about data.\u201d</p> <p>It provides information that describes, explains, or gives context to other data.</p> <p>Examples of Metadata:</p> <p>Table name</p> <p>Column names</p> <p>Data types</p> <p>File location</p> <p>Owner</p> <p>Created date</p> <p>Permissions</p> <p>Metadata helps in:</p> <p>Data discovery</p> <p>Governance</p> <p>Access control</p> <p>Query optimization</p> <p>4\ufe0f\u20e3 Managed Tables vs External Tables</p> <p>Databricks supports two main types of tables:</p> <p>\ud83d\udd39 Managed Tables</p> <p>In managed tables, Databricks manages both metadata and physical data storage.</p> <p>Characteristics:</p> <p>Storage location controlled by Databricks</p> <p>Dropping table deletes both metadata and data</p> <p>Strong governance using Unity Catalog</p> <p>Suitable for fully controlled environments</p> <p>Multi-tool Access: Difficult</p> <p>Data Governance: Fully governed</p> <p>Use Case: Quick analytics, internal BI systems, tightly controlled environments</p> <p>\ud83d\udd39 External Tables</p> <p>In external tables, Databricks manages only metadata, while data remains in external storage (like S3, ADLS, GCS).</p> <p>Characteristics:</p> <p>Data stored outside Databricks-managed location</p> <p>Dropping table removes only metadata</p> <p>Flexible integration with other tools</p> <p>Requires governance discipline</p> <p>Multi-tool Access: Easy</p> <p>Data Governance: Flexible but requires discipline</p> <p>Use Case: Shared datasets, existing data lakes, multi-tool ecosystems</p>"},{"location":"DataBricks/introduction/#managed-vs-external-tables","title":"\ud83d\udd0e Managed vs External Tables","text":"Feature Managed Table External Table Metadata Management Databricks Databricks Data Storage Managed by Databricks Stored externally (S3/ADLS/GCS) Data Deletion Metadata + Data deleted Only metadata deleted Multi-tool Access Limited Easy Governance Fully controlled Flexible Best For Internal analytics Shared datasets / Existing data lakes <p>5\ufe0f\u20e3 Lakehouse Architecture</p> <p>Databricks implements the Lakehouse Architecture, which combines:</p> <p>Data Lake   Data Warehouse</p> <p>Cheap storage   High performance</p> <p>Flexible schema Structured governance</p> <p>Raw data storage    BI-ready data</p> <p>Lakehouse provides:</p> <p>ACID transactions</p> <p>Schema enforcement</p> <p>Time travel</p> <p>Batch + Streaming support</p>"},{"location":"DataBricks/introduction/#lakehouse-architecture-overview","title":"\ud83d\udcca Lakehouse Architecture Overview","text":"<p>6\ufe0f\u20e3 \ud83c\udfd7\ufe0f Medallion Architecture (Lakehouse Design Pattern)</p> <p>The Medallion Architecture is a data design pattern used in Databricks to organize data into three layers:</p>"},{"location":"DataBricks/introduction/#medallion-architecture-and-unity-catalog-overview","title":"\ud83c\udfd7\ufe0f Medallion architecture and unity catalog overview","text":"<p>\ud83e\udd49 Bronze Layer \u2013 Raw Data</p> <p>Ingested data from source systems Minimal transformation Used for auditing and traceability</p> <p>\ud83e\udd48 Silver Layer \u2013 Cleaned &amp; Transformed Data</p> <p>Data cleaning Deduplication Standardization Schema enforcement</p> <p>\ud83e\udd47 Gold Layer \u2013 Business-Level Data</p> <p>Aggregated and curated datasets Business KPIs Optimized for reporting and analytics</p> <p>This layered approach improves:</p> <p>Data quality Maintainability Performance Governance</p>"},{"location":"DataBricks/introduction/#bronze-vs-silver-vs-gold","title":"\ud83d\udcca Bronze vs Silver vs Gold","text":"Layer Purpose Data Quality Transformation Level Used By Bronze Raw ingestion Low Minimal Data Engineers Silver Cleaned &amp; standardized Medium Moderate Data Engineers / Analysts Gold Business-ready data High Aggregated &amp; Curated BI / Business Users <p>Source \u2192 Bronze \u2192 Silver \u2192 Gold \u2192 BI / ML</p> <p>7\ufe0f\u20e3 Delta Lake</p> <p>Delta Lake is the storage layer of Databricks that adds reliability to data lakes.</p> <p>Delta Lake is a open-source storage layer that brings ACID transactions, schema enforcement and time travel capabilities to data lakes.</p> <p>It provides:</p> <p>ACID transactions</p> <p>Schema enforcement</p> <p>Schema evolution</p> <p>Time travel (versioning)</p> <p>Scalable metadata handling</p> <p>Delta Lake solves common data lake problems such as:</p> <p>Dirty reads</p> <p>Data corruption</p> <p>Concurrent write issues</p> <p>8\ufe0f\u20e3 Unity Catalog</p> <p>Unity Catalog is Databricks\u2019 unified governance solution for data and AI assets.</p> <p>It manages:</p> <p>Tables</p> <p>Views</p> <p>Files</p> <p>ML models</p> <p>Permissions</p> <p>Lineage tracking</p> <p>Benefits:</p> <p>Centralized access control</p> <p>Fine-grained permissions (row/column level)</p> <p>Data lineage tracking</p> <p>Cross-workspace governance</p> <p>It ensures secure and compliant data usage across the organization.</p> <p>9\ufe0f\u20e3 ACID Principles</p> <p>Databricks (via Delta Lake) supports ACID properties:</p> <p>A \u2013 Atomicity A transaction either fully completes or fully fails.</p> <p>C \u2013 Consistency Data remains valid before and after a transaction.</p> <p>I \u2013 Isolation Concurrent transactions do not interfere with each other.</p> <p>D \u2013 Durability Once committed, data remains stored even if failures occur.</p> <p>ACID ensures reliability in large-scale data systems.</p>"},{"location":"DataBricks/introduction/#acid-properties-in-delta-lake","title":"\ud83d\udd12 ACID Properties in Delta Lake","text":"Property Meaning Example Atomicity All or nothing execution Failed transaction rolls back Consistency Data remains valid Constraints enforced Isolation Transactions do not interfere Concurrent writes handled safely Durability Data remains after commit Data persists after crash <p>\ud83d\udd39 OLTP vs OLAP (Very Important for Data Engineers)</p> <p>OLTP is for running daily business transactions, while OLAP is for analyzing data and generating insights.</p> <p>Since you're moving towards Data Engineering / Analytics, understanding this clearly is very important.</p> <p>\ud83d\udfe2 What is OLTP?</p> <p>OLTP = Online Transaction Processing</p> <p>\ud83d\udc49 Used for day-to-day operations \ud83d\udc49 Handles many small transactions</p> <p>\ud83c\udfe6 Example Systems</p> <p>Amazon (placing an order)</p> <p>Paytm (making payment)</p> <p>Bank ATM withdrawal</p> <p>\ud83d\udccc What Happens in OLTP?</p> <p>When you:</p> <p>Add an item to cart</p> <p>Make payment</p> <p>Transfer money</p> <p>Book a ticket</p> <p>The database:</p> <p>Inserts data</p> <p>Updates records</p> <p>Deletes records</p> <p>Ensures data consistency</p> <p>\ud83d\uddbc OLTP Database Structure</p> <p></p> <p>\u2714 Highly normalized tables \u2714 Fast inserts &amp; updates \u2714 Supports thousands of concurrent users</p> <p>\ud83d\udd39 Example OLTP Query:</p> <p>UPDATE orders SET status = 'Shipped' WHERE order_id = 101;</p> <p>Small, fast transaction.</p> <p>\ud83d\udd35 What is OLAP?</p> <p>OLAP = Online Analytical Processing</p> <p>\ud83d\udc49 Used for analysis &amp; reporting \ud83d\udc49 Works on large historical data</p> <p>Used by:</p> <p>Data Analysts</p> <p>Data Scientists</p> <p>Business Intelligence teams</p> <p>\ud83d\udcca Example Systems</p> <p>Tableau</p> <p>Power BI</p> <p>Snowflake</p> <p>Databricks</p> <p>\ud83d\uddbc OLAP Structure (Data Warehouse)</p> <p> </p> <p>\u2714 Star schema</p> <p>\u2714 Fact table + Dimension tables</p> <p>\u2714 Aggregations</p> <p>\u2714 Historical data</p> <p>\ud83d\udd39 Example OLAP Query</p> <p>SELECT region, SUM(sales) FROM sales_data GROUP BY region;</p> <p>This scans millions of rows.</p>"},{"location":"DataBricks/introduction/#oltp-vs-olap-comparison-table","title":"\u2696 OLTP vs OLAP (Comparison Table)","text":"Feature OLTP (Online Transaction Processing) OLAP (Online Analytical Processing) Purpose Run business operations Analyze business data Users Customers, application users Analysts, data scientists Data Current / real-time data Historical data Queries Simple (INSERT, UPDATE, DELETE) Complex (aggregations, joins) Speed Milliseconds Seconds / Minutes Schema Normalized Star / Snowflake schema Example DB MySQL, PostgreSQL Snowflake, BigQuery, Redshift <p>\ud83e\udde0 Simple Real-Life Example</p> <p>Think of a supermarket:</p> <p>\ud83d\uded2 Billing counter \u2192 OLTP</p> <p>\ud83d\udcca Monthly sales analysis \u2192 OLAP</p> <p>Since you:</p> <p>Work in analytics</p> <p>Want Data Engineer / BI roles</p> <p>\ud83d\udc49 You will mainly work with OLAP systems \ud83d\udc49 But you must understand OLTP to design pipelines</p> <p>OLTP vs OLAP Overview</p> <p>OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing) are two different types of database systems designed for different purposes.</p> <p>Key Difference: OLTP is used to run daily business operations. OLAP is used to analyze data and generate insights.</p> <p>OLTP (Online Transaction Processing) Definition</p> <p>OLTP systems are designed to manage real-time business transactions. They handle a large number of small, fast operations such as insert, update, and delete.</p> <p>Characteristics</p> <p>Highly normalized tables (3NF)</p> <p>Fast insert/update/delete</p> <p>Ensures ACID properties</p> <p>Handles thousands of transactions per second</p> <p>Small queries</p> <p>OLTP Architecture (Conceptual View)</p> <p>OLAP (Online Analytical Processing) Definition</p> <p>OLAP systems are designed for complex queries and data analysis. They work on large volumes of historical data.</p> <p>Characteristics :</p> <p>Star or Snowflake schema</p> <p>Fact and dimension tables</p> <p>Handles large datasets</p> <p>Complex aggregations</p> <p>Read-heavy workload</p> <p>Simple Analogy</p> <p>Supermarket example:</p> <p>\ud83d\uded2 Billing counter \u2192 OLTP</p> <p>\ud83d\udcca Monthly sales dashboard \u2192 OLAP</p> <p>Important for Data Engineers</p> <p>OLTP \u2192 Source systems</p> <p>ETL/ELT \u2192 Moves data</p> <p>OLAP \u2192 Data warehouse / Analytics system</p> <p>Understanding both is critical for designing data pipelines.</p> <p>Hirarchy in Databricks.</p> <p>Workspace &gt; Catalog &gt; Schema &gt; Tables</p> <p>\ud83d\udd1f Summary</p> <p>Databricks is a modern data platform that:</p> <p>Implements Lakehouse architecture</p> <p>Uses Delta Lake for reliability</p> <p>Supports Medallion architecture for structured data processing</p> <p>Provides centralized governance via Unity Catalog</p> <p>Enables scalable data engineering, analytics, and AI workloads</p> <p>It simplifies big data processing while maintaining enterprise-grade governance and reliability.</p>"},{"location":"DataEngineering/de-cycle/","title":"Data Engineering Lifecycle","text":""},{"location":"DataEngineering/de-cycle/#1-data-generation","title":"1. Data Generation","text":"<p>Applications, APIs, logs.</p>"},{"location":"DataEngineering/de-cycle/#2-data-ingestion","title":"2. Data Ingestion","text":"<p>Batch or Streaming.</p>"},{"location":"DataEngineering/de-cycle/#3-storage","title":"3. Storage","text":"<p>Data Lake, Warehouse, Lakehouse.</p>"},{"location":"DataEngineering/de-cycle/#4-processing","title":"4. Processing","text":"<p>Spark, SQL, Python.</p>"},{"location":"DataEngineering/de-cycle/#5-serving","title":"5. Serving","text":"<p>BI tools and ML models.</p>"},{"location":"DataEngineering/de-cycle/#test-data","title":"Test data","text":""},{"location":"DataScience/machine-learning/","title":"ML Algorithms","text":""}]}