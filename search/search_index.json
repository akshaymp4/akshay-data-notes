{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"AWS/aws_notes/","title":"Amazon Web Service","text":""},{"location":"AWS/aws_notes/#welcome-to-cloud-for-data-scineceanalyticsengineering","title":"Welcome to Cloud for Data Scinece/Analytics/Engineering","text":"<p>Amazon Web Services (AWS) for Data Science</p> <p>1\ufe0f\u20e3 What is AWS?</p> <p>Definition</p> <p>Amazon Web Services (AWS) is a cloud computing platform by Amazon that provides scalable infrastructure and services over the internet.</p> <p>Instead of buying physical servers, you can: Rent virtual machines Store data Deploy applications Build ML models All on-demand.</p> <p>2\ufe0f\u20e3 Why AWS for Data Science?</p> <p>Traditional Setup Problem : </p> <p>Buy servers</p> <p>Configure networking</p> <p>Maintain hardware</p> <p>Scale manually</p> <p>High upfront cost</p> <p>AWS Solution</p> <p>No hardware Instant server creation</p> <p>Auto scaling</p> <p>Pay only for usage</p> <p>Managed ML services</p> <p>3\ufe0f\u20e3 Core AWS Concepts</p>"},{"location":"AWS/aws_notes/#31-cloud-computing","title":"\u2601\ufe0f 3.1 Cloud Computing","text":"<p>Definition:</p> <p>Delivering computing services over the internet.</p> <p>Types:</p> <p>IaaS \u2013 Infrastructure as a Service (EC2)</p> <p>PaaS \u2013 Platform as a Service (Elastic Beanstalk)</p> <p>SaaS \u2013 Software as a Service (Gmail, Zoom)</p> <p>\ud83c\udf0d 3.2 Regions &amp; Availability Zones</p> <p>A Region is a geographic area where AWS has data centers.</p> <p>Example: Mumbai (ap-south-1), US-East (Virginia).</p> <p>Each Region contains multiple Availability Zones (AZs).</p> <p>Why this matters:</p> <p>If one data center fails \u2192 other AZ works.</p> <p>Used for high availability.</p> <p>Critical for production systems.</p> <p>4\ufe0f\u20e3 AWS Storage Services</p> <p>4.1 Amazon S3 (Simple Storage Service)</p> <p> </p> <p>Definition:</p> <p>Object storage service used to store files.</p> <p>Key Concepts:</p> <p>Bucket \u2192 Folder</p> <p>Object \u2192 File</p> <p>Unlimited storage</p> <p>Highly durable (99.999999999%)</p> <p>Use Cases:</p> <p>Store CSV datasets</p> <p>Store ML models</p> <p>Store backups</p> <p>Host static websites</p> <p>\ud83d\udcbe 4.2 EBS (Elastic Block Store)</p> <p>Definition:</p> <p>Block storage attached to EC2.</p> <p>Works like Hard Disk</p> <p>Used for OS, database storage</p> <p>Persistent storage</p> <p>\ud83e\uddca 4.3 Glacier</p> <p>Definition:</p> <p>Low-cost storage for archives. Cheap Slow retrieval Used for backups</p> <p>5\ufe0f\u20e3 AWS Compute Services</p> <p>\ud83d\udda5 5.1 EC2 (Elastic Compute Cloud)</p> <p> </p> <p>Definition:</p> <p>EC2 instances come in different types:</p> <p>General Purpose \u2192 t2, t3 (small apps, freelancing)</p> <p>Compute Optimized \u2192 c5 (ML training)</p> <p>Memory Optimized \u2192 r5 (big datasets)</p> <p>GPU \u2192 p3, g4 (Deep learning)</p> <p>You choose instance type based on:</p> <p>RAM requirement</p> <p>CPU cores</p> <p>GPU need</p> <p>Budget</p> <p>You can:</p> <p>Install Python</p> <p>Install MySQL</p> <p>Run ML models</p> <p>Host applications</p> <p>Key Components:</p> <p>AMI (Amazon Machine Image) \u2192 OS template</p> <p>Instance Type \u2192 CPU/RAM configuration</p> <p>Key Pair \u2192 SSH access</p> <p>Security Group \u2192 Firewall</p> <p>EC2 for Data Science:</p> <p>Create Ubuntu server</p> <p>Install Anaconda</p> <p>Connect via SSH</p> <p>Run Jupyter Notebook</p> <p>\u26a1 5.2 AWS Lambda</p> <p>Definition:</p> <p>Serverless compute service.</p> <p>No server management</p> <p>Event-driven</p> <p>Pay per execution</p> <p>Used for:</p> <p>Triggering pipeline jobs</p> <p>Automating workflows</p>"},{"location":"AWS/aws_notes/#6-networking-basics","title":"6\ufe0f\u20e3 Networking Basics","text":"<p>\ud83c\udf10 6.1 VPC (Virtual Private Cloud)</p> <p>Definition:</p> <p>Create public servers (accessible from internet)</p> <p>Create private servers (internal only)</p> <p>Control IP ranges</p> <p>Secure databases</p> <p>Example:</p> <p>EC2 in public subnet</p> <p>RDS in private subnet</p> <p>You control:</p> <p>IP ranges</p> <p>Subnets</p> <p>Routing</p> <p>Security</p> <p>\ud83d\udd10 Security Group</p> <p>Acts as:</p> <p>Firewall for EC2</p> <p>Controls:</p> <p>Inbound traffic</p> <p>Outbound traffic</p> <p>Example:</p> <p>Allow port 22 for SSH</p> <p>Allow port 8888 for Jupyter</p> <p>7\ufe0f\u20e3 AWS CLI</p> <p>Definition:</p> <p>Command Line Interface to control AWS from terminal.</p> <p>Example:</p> <p>aws s3 ls</p> <p>aws ec2 describe-instances</p> <p>Used for:</p> <p>Automation</p> <p>DevOps</p> <p>CI/CD</p> <p>8\ufe0f\u20e3 Application Deployment in AWS</p> <p>Deployment Methods:</p> <p>1\ufe0f\u20e3 Manual EC2 Deployment</p> <p>Launch EC2</p> <p>Install software</p> <p>Upload code</p> <p>Run app</p> <p>2\ufe0f\u20e3 Elastic Beanstalk</p> <p>Upload code</p> <p>AWS manages infrastructure</p> <p>3\ufe0f\u20e3 Docker + EC2</p> <p>Container-based deployment</p>"},{"location":"AWS/aws_notes/#9-aws-sagemaker-for-machine-learning","title":"9\ufe0f\u20e3 AWS SageMaker (For Machine Learning)","text":"<p>Definition:</p> <p>SageMaker is a fully managed ML platform.</p> <p>It removes the need to:</p> <p>Launch EC2 manually</p> <p>Configure GPUs</p> <p>Install ML libraries</p> <p>It provides:</p> <p>Notebook environment</p> <p>Built-in algorithms</p> <p>Distributed training</p> <p>One-click deployment</p> <p>Model monitoring</p> <p>You can:</p> <p>Build models</p> <p>Train models</p> <p>Deploy models</p> <p>Monitor models</p> <p>SageMaker Workflow</p> <p>Upload data to S3</p> <p>Create Notebook instance</p> <p>Train model</p> <p>Deploy endpoint</p> <p>Use API for predictions</p> <p>Why SageMaker?</p> <p>No server setup</p> <p>Auto scaling</p> <p>Built-in algorithms</p> <p>Production ready</p>"},{"location":"AWS/aws_notes/#iam-identity-and-access-management","title":"\ud83d\udd1f IAM (Identity and Access Management)","text":"<p>Definition:</p> <p>IAM controls access inside AWS.</p> <p>Components:</p> <p>User \u2192 Person</p> <p>Role \u2192 Permission set assigned to service</p> <p>Policy \u2192 JSON document defining allowed actions</p> <p>Example:</p> <p>Give EC2 permission to read S3</p> <p>Restrict intern from deleting resources</p> <p>You can:</p> <p>Create users, Assign roles, Attach policies</p> <p>Important for:</p> <p>Security</p> <p>Controlled access</p>"},{"location":"AWS/aws_notes/#11-databases-in-aws","title":"1\ufe0f\u20e31\ufe0f\u20e3 Databases in AWS","text":"<p>RDS (Relational Database Service)</p> <p>Definition:</p> <p>Managed SQL database.</p> <p>Supports:</p> <p>MySQL</p> <p>PostgreSQL</p> <p>MariaDB</p> <p>No need to manage:</p> <p>Backups</p> <p>Scaling</p> <p>Patching</p>"},{"location":"AWS/aws_notes/#12-data-science-architecture-beginner-level","title":"1\ufe0f\u20e32\ufe0f\u20e3 Data Science Architecture (Beginner Level)","text":"<p>Example Workflow:</p> <p>Store raw data \u2192 S3</p> <p>Launch EC2</p> <p>Connect to S3</p> <p>Train model</p> <p>Save model in S3</p> <p>Deploy via EC2 or SageMaker</p> <p>1\ufe0f\u20e33\ufe0f\u20e3 Pay-As-You-Go Model</p> <p>AWS charges based on:</p> <p>Compute hours</p> <p>Storage used</p> <p>Data transfer</p> <p>API calls</p> <p>No upfront cost.</p> <p>1\ufe0f\u20e34\ufe0f\u20e3 When to Use What?</p> <p>Requirement Service</p> <p>Store datasets  S3</p> <p>Run Python code EC2</p> <p>Serverless automation   Lambda</p> <p>Train ML models easily  SageMaker</p> <p>SQL database    RDS</p> <p>Archive data    Glacier</p>"},{"location":"AWS/aws_notes/#15-advantages-of-aws","title":"1\ufe0f\u20e35\ufe0f\u20e3 Advantages of AWS","text":"<p>Scalable</p> <p>Reliable</p> <p>Secure</p> <p>Global infrastructure</p> <p>Large ecosystem</p> <p>Free Tier available</p>"},{"location":"AWS/aws_notes/#final-summary","title":"\ud83d\ude80 Final Summary","text":"<p>AWS provides:</p> <p>Storage (S3)</p> <p>Compute (EC2, Lambda)</p> <p>ML Platform (SageMaker)</p> <p>Databases (RDS)</p> <p>Networking (VPC)</p> <p>Security (IAM)</p> <p>It allows data scientists to:</p>"},{"location":"AWS/aws_notes/#aws-products-with-normal-equivalent","title":"\u2705 AWS Products with Normal Equivalent","text":"<p>This is what you asked clearly \ud83d\udc4d</p>"},{"location":"AWS/aws_notes/#aws-products-and-their-normal-equivalent","title":"AWS Products and Their Normal Equivalent","text":"AWS Product Category What It Is Normal Equivalent (Traditional Setup) EC2 Compute Virtual machine in cloud Physical server / Your laptop S3 Object Storage Stores files (objects) Google Drive / External storage EBS Block Storage Hard disk attached to EC2 Internal HDD / SSD RDS Relational Database Managed SQL database MySQL installed on server DynamoDB NoSQL Database Managed NoSQL database MongoDB Redshift Data Warehouse Analytical database for BI &amp; reporting Snowflake / On-prem Data Warehouse Athena Query Engine Query S3 using SQL Presto / Querying CSV locally Glue ETL Service Data pipeline &amp; transformation service Talend / Manual Python ETL scripts SageMaker ML Platform Build, train &amp; deploy ML models Jupyter + Flask + Manual deployment Lambda Serverless Compute Run code without managing servers Cron job / Background script Elastic Beanstalk PaaS Easy application deployment Heroku VPC Networking Private network inside AWS Office LAN network"},{"location":"AWS/aws_notes/#database-vs-data-warehouse-vs-data-lake","title":"Database vs Data Warehouse vs Data Lake","text":"Feature Database (OLTP) Data Warehouse (OLAP) Data Lake Main Purpose Daily transactions Business analytics Store raw data Data Type Structured Structured Structured + Unstructured Example AWS RDS / DynamoDB Redshift S3 Query Type Simple queries Complex analytical queries Process after storing Schema Schema-on-write Schema-on-write Schema-on-read Data Volume Medium Large Very Large Users Application systems Analysts / BI tools Data Engineers / Scientists Cost Moderate Higher Cheapest storage"},{"location":"DataBricks/introduction/","title":"1\ufe0f\u20e3 Introduction","text":"<p>\ud83d\udcd8 What is Databricks?</p> <p>Databricks is a cloud-based unified data and AI platform built on top of Apache Spark that enables organizations to process, analyze, and build machine learning solutions on large-scale data.</p> <p>It is designed around the Lakehouse architecture, which combines:</p> <p>Data Lake capabilities \u2192 Low-cost storage, flexibility, support for structured and unstructured data</p> <p>Data Warehouse capabilities \u2192 High performance, ACID transactions, governance, and BI optimization</p> <p>This means Databricks gives you the flexibility of a data lake and the reliability of a data warehouse in a single platform.</p> <p>\ud83d\ude80 Why Databricks is Called a Managed Service</p> <p>Databricks is a managed service, meaning you do not need to manually set up or maintain infrastructure.</p> <p>Instead of configuring servers, installing Spark, handling failures, and tuning performance \u2014 Databricks manages these for you automatically.</p> <p>It takes care of:</p> <p>Cluster management Infrastructure provisioning Scaling Performance optimization Security integrations</p> <p>This allows:</p> <p>Data Engineers \u2192 to focus on pipelines</p> <p>Analysts \u2192 to focus on insights</p> <p>Data Scientists \u2192 to focus on models</p> <p>Instead of spending time on DevOps or infrastructure setup.</p> <p>1\ufe0f\u20e3 Cluster Management</p> <p>Cluster management refers to the automatic creation, configuration, monitoring, and termination of compute clusters used to process data.</p> <p>\ud83d\udc49 Databricks automatically manages Spark clusters so users don\u2019t need to manually configure servers.</p> <p>2\ufe0f\u20e3 Infrastructure Provisioning</p> <p>Infrastructure provisioning is the process of setting up cloud resources such as virtual machines, storage, and networking.</p> <p>\ud83d\udc49 Databricks automatically provisions the required cloud infrastructure (AWS, Azure, GCP) when you start a cluster.</p> <p>3\ufe0f\u20e3 Scaling</p> <p>Scaling is the ability to increase or decrease computing resources based on workload demand.</p> <p>\ud83d\udc49 Databricks supports auto-scaling, meaning it can add or remove worker nodes automatically depending on workload size.</p> <p>4\ufe0f\u20e3 Performance Optimization</p> <p>Performance optimization involves tuning system resources and execution strategies to run workloads faster and more efficiently.</p> <p>\ud83d\udc49 Databricks optimizes Spark jobs automatically using features like query optimization, caching, and optimized execution engines.</p> <p>5\ufe0f\u20e3 Security Integrations</p> <p>Security integrations ensure that data access and system usage are secure and compliant with organizational policies.</p> <p>\ud83d\udc49 Databricks integrates with cloud IAM systems, role-based access control, encryption, and Unity Catalog for governance.</p> <p>\ud83d\udca1 In Simple Words </p> <p>Without Databricks \u2192 You manage servers, install Spark, scale manually, secure everything yourself.</p> <p>With Databricks \u2192 You just write code. The platform manages everything else</p> <p>This allows data engineers, analysts, and data scientists to focus on building data solutions instead of managing infrastructure.</p>"},{"location":"DataBricks/introduction/#lakehouse-architecture-workflow-diagram","title":"Lakehouse architecture workflow diagram","text":"<p>2\ufe0f\u20e3 Key Components of Databricks</p> <p>Workspaces \u2013 Collaborative environment for notebooks, jobs, and dashboards</p> <p>Clusters \u2013 Compute resources to run Spark workloads</p> <p>DBFS (Databricks File System) \u2013 Distributed file system abstraction</p> <p>Delta Lake \u2013 Storage layer providing ACID transactions</p> <p>Unity Catalog \u2013 Centralized governance layer</p> <p>Competetors of Delta lake are : </p> <pre><code>Apache Iceberg, \nApache Hudi, \nSnowflake, \nMicrosoft Fabric (OneLake)\n</code></pre>"},{"location":"DataBricks/introduction/#data-lake-vs-data-warehouse-vs-lakehouse","title":"\ud83d\udd0e Data Lake vs Data Warehouse vs Lakehouse","text":"Feature Data Lake Data Warehouse Lakehouse (Databricks) Storage Cost Low High Low Schema Flexible Structured Structured + Flexible Performance Medium High High ACID Support \u274c No \u2705 Yes \u2705 Yes (Delta Lake) Governance Limited Strong Strong (Unity Catalog) Supports ML \u2705 Yes Limited \u2705 Yes <p>3\ufe0f\u20e3 What is Metadata?</p> <p>Metadata is \u201cdata about data.\u201d</p> <p>It provides information that describes, explains, or gives context to other data.</p> <p>Examples of Metadata:</p> <p>Table name</p> <p>Column names</p> <p>Data types</p> <p>File location</p> <p>Owner</p> <p>Created date</p> <p>Permissions</p> <p>Metadata helps in:</p> <p>Data discovery</p> <p>Governance</p> <p>Access control</p> <p>Query optimization</p> <p>4\ufe0f\u20e3 Managed Tables vs External Tables</p> <p>Databricks supports two main types of tables:</p> <p>\ud83d\udd39 Managed Tables</p> <p>In managed tables, Databricks manages both metadata and physical data storage.</p> <p>Characteristics:</p> <p>Storage location controlled by Databricks</p> <p>Dropping table deletes both metadata and data</p> <p>Strong governance using Unity Catalog</p> <p>Suitable for fully controlled environments</p> <p>Multi-tool Access: Difficult</p> <p>Data Governance: Fully governed</p> <p>Use Case: Quick analytics, internal BI systems, tightly controlled environments</p> <p>\ud83d\udd39 External Tables</p> <p>In external tables, Databricks manages only metadata, while data remains in external storage (like S3, ADLS, GCS).</p> <p>Characteristics:</p> <p>Data stored outside Databricks-managed location</p> <p>Dropping table removes only metadata</p> <p>Flexible integration with other tools</p> <p>Requires governance discipline</p> <p>Multi-tool Access: Easy</p> <p>Data Governance: Flexible but requires discipline</p> <p>Use Case: Shared datasets, existing data lakes, multi-tool ecosystems</p>"},{"location":"DataBricks/introduction/#managed-vs-external-tables","title":"\ud83d\udd0e Managed vs External Tables","text":"Feature Managed Table External Table Metadata Management Databricks Databricks Data Storage Managed by Databricks Stored externally (S3/ADLS/GCS) Data Deletion Metadata + Data deleted Only metadata deleted Multi-tool Access Limited Easy Governance Fully controlled Flexible Best For Internal analytics Shared datasets / Existing data lakes <p>5\ufe0f\u20e3 Lakehouse Architecture</p> <p>Databricks implements the Lakehouse Architecture, which combines:</p> <p>Data Lake   Data Warehouse</p> <p>Cheap storage   High performance</p> <p>Flexible schema Structured governance</p> <p>Raw data storage    BI-ready data</p> <p>Lakehouse provides:</p> <p>ACID transactions</p> <p>Schema enforcement</p> <p>Time travel</p> <p>Batch + Streaming support</p>"},{"location":"DataBricks/introduction/#lakehouse-architecture-overview","title":"\ud83d\udcca Lakehouse Architecture Overview","text":"<p>6\ufe0f\u20e3 \ud83c\udfd7\ufe0f Medallion Architecture (Lakehouse Design Pattern)</p> <p>The Medallion Architecture is a data design pattern used in Databricks to organize data into three layers:</p>"},{"location":"DataBricks/introduction/#medallion-architecture-and-unity-catalog-overview","title":"\ud83c\udfd7\ufe0f Medallion architecture and unity catalog overview","text":"<p>\ud83e\udd49 Bronze Layer \u2013 Raw Data</p> <p>Ingested data from source systems Minimal transformation Used for auditing and traceability</p> <p>\ud83e\udd48 Silver Layer \u2013 Cleaned &amp; Transformed Data</p> <p>Data cleaning Deduplication Standardization Schema enforcement</p> <p>\ud83e\udd47 Gold Layer \u2013 Business-Level Data</p> <p>Aggregated and curated datasets Business KPIs Optimized for reporting and analytics</p> <p>This layered approach improves:</p> <p>Data quality Maintainability Performance Governance</p>"},{"location":"DataBricks/introduction/#bronze-vs-silver-vs-gold","title":"\ud83d\udcca Bronze vs Silver vs Gold","text":"Layer Purpose Data Quality Transformation Level Used By Bronze Raw ingestion Low Minimal Data Engineers Silver Cleaned &amp; standardized Medium Moderate Data Engineers / Analysts Gold Business-ready data High Aggregated &amp; Curated BI / Business Users <p>Source \u2192 Bronze \u2192 Silver \u2192 Gold \u2192 BI / ML</p> <p>7\ufe0f\u20e3 Delta Lake</p> <p>Delta Lake is the storage layer of Databricks that adds reliability to data lakes.</p> <p>Delta Lake is a open-source storage layer that brings ACID transactions, schema enforcement and time travel capabilities to data lakes.</p> <p>It provides:</p> <p>ACID transactions</p> <p>Schema enforcement</p> <p>Schema evolution</p> <p>Time travel (versioning)</p> <p>Scalable metadata handling</p> <p>Delta Lake solves common data lake problems such as:</p> <p>Dirty reads</p> <p>Data corruption</p> <p>Concurrent write issues</p> <p>8\ufe0f\u20e3 Unity Catalog</p> <p>Unity Catalog is Databricks\u2019 unified governance solution for data and AI assets.</p> <p>It manages:</p> <p>Tables</p> <p>Views</p> <p>Files</p> <p>ML models</p> <p>Permissions</p> <p>Lineage tracking</p> <p>Benefits:</p> <p>Centralized access control</p> <p>Fine-grained permissions (row/column level)</p> <p>Data lineage tracking</p> <p>Cross-workspace governance</p> <p>It ensures secure and compliant data usage across the organization.</p> <p>9\ufe0f\u20e3 ACID Principles</p> <p>Databricks (via Delta Lake) supports ACID properties:</p> <p>A \u2013 Atomicity A transaction either fully completes or fully fails.</p> <p>C \u2013 Consistency Data remains valid before and after a transaction.</p> <p>I \u2013 Isolation Concurrent transactions do not interfere with each other.</p> <p>D \u2013 Durability Once committed, data remains stored even if failures occur.</p> <p>ACID ensures reliability in large-scale data systems.</p>"},{"location":"DataBricks/introduction/#acid-properties-in-delta-lake","title":"\ud83d\udd12 ACID Properties in Delta Lake","text":"Property Meaning Example Atomicity All or nothing execution Failed transaction rolls back Consistency Data remains valid Constraints enforced Isolation Transactions do not interfere Concurrent writes handled safely Durability Data remains after commit Data persists after crash <p>\ud83d\udd39 OLTP vs OLAP (Very Important for Data Engineers)</p> <p>OLTP is for running daily business transactions, while OLAP is for analyzing data and generating insights.</p> <p>Since you're moving towards Data Engineering / Analytics, understanding this clearly is very important.</p> <p>\ud83d\udfe2 What is OLTP?</p> <p>OLTP = Online Transaction Processing</p> <p>\ud83d\udc49 Used for day-to-day operations \ud83d\udc49 Handles many small transactions</p> <p>\ud83c\udfe6 Example Systems</p> <p>Amazon (placing an order)</p> <p>Paytm (making payment)</p> <p>Bank ATM withdrawal</p> <p>\ud83d\udccc What Happens in OLTP?</p> <p>When you:</p> <p>Add an item to cart</p> <p>Make payment</p> <p>Transfer money</p> <p>Book a ticket</p> <p>The database:</p> <p>Inserts data</p> <p>Updates records</p> <p>Deletes records</p> <p>Ensures data consistency</p> <p>\ud83d\uddbc OLTP Database Structure</p> <p></p> <p>\u2714 Highly normalized tables \u2714 Fast inserts &amp; updates \u2714 Supports thousands of concurrent users</p> <p>\ud83d\udd39 Example OLTP Query:</p> <p>UPDATE orders SET status = 'Shipped' WHERE order_id = 101;</p> <p>Small, fast transaction.</p> <p>\ud83d\udd35 What is OLAP?</p> <p>OLAP = Online Analytical Processing</p> <p>\ud83d\udc49 Used for analysis &amp; reporting \ud83d\udc49 Works on large historical data</p> <p>Used by:</p> <p>Data Analysts</p> <p>Data Scientists</p> <p>Business Intelligence teams</p> <p>\ud83d\udcca Example Systems</p> <p>Tableau</p> <p>Power BI</p> <p>Snowflake</p> <p>Databricks</p> <p>\ud83d\uddbc OLAP Structure (Data Warehouse)</p> <p> </p> <p>\u2714 Star schema</p> <p>\u2714 Fact table + Dimension tables</p> <p>\u2714 Aggregations</p> <p>\u2714 Historical data</p> <p>\ud83d\udd39 Example OLAP Query</p> <p>SELECT region, SUM(sales) FROM sales_data GROUP BY region;</p> <p>This scans millions of rows.</p>"},{"location":"DataBricks/introduction/#oltp-vs-olap-comparison-table","title":"\u2696 OLTP vs OLAP (Comparison Table)","text":"Feature OLTP (Online Transaction Processing) OLAP (Online Analytical Processing) Purpose Run business operations Analyze business data Users Customers, application users Analysts, data scientists Data Current / real-time data Historical data Queries Simple (INSERT, UPDATE, DELETE) Complex (aggregations, joins) Speed Milliseconds Seconds / Minutes Schema Normalized Star / Snowflake schema Example DB MySQL, PostgreSQL Snowflake, BigQuery, Redshift <p>\ud83e\udde0 Simple Real-Life Example</p> <p>Think of a supermarket:</p> <p>\ud83d\uded2 Billing counter \u2192 OLTP</p> <p>\ud83d\udcca Monthly sales analysis \u2192 OLAP</p> <p>Since you:</p> <p>Work in analytics</p> <p>Want Data Engineer / BI roles</p> <p>\ud83d\udc49 You will mainly work with OLAP systems \ud83d\udc49 But you must understand OLTP to design pipelines</p> <p>OLTP vs OLAP Overview</p> <p>OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing) are two different types of database systems designed for different purposes.</p> <p>Key Difference: OLTP is used to run daily business operations. OLAP is used to analyze data and generate insights.</p> <p>OLTP (Online Transaction Processing) Definition</p> <p>OLTP systems are designed to manage real-time business transactions. They handle a large number of small, fast operations such as insert, update, and delete.</p> <p>Characteristics</p> <p>Highly normalized tables (3NF)</p> <p>Fast insert/update/delete</p> <p>Ensures ACID properties</p> <p>Handles thousands of transactions per second</p> <p>Small queries</p> <p>OLTP Architecture (Conceptual View)</p> <p>OLAP (Online Analytical Processing) Definition</p> <p>OLAP systems are designed for complex queries and data analysis. They work on large volumes of historical data.</p> <p>Characteristics :</p> <p>Star or Snowflake schema</p> <p>Fact and dimension tables</p> <p>Handles large datasets</p> <p>Complex aggregations</p> <p>Read-heavy workload</p> <p>Simple Analogy</p> <p>Supermarket example:</p> <p>\ud83d\uded2 Billing counter \u2192 OLTP</p> <p>\ud83d\udcca Monthly sales dashboard \u2192 OLAP</p> <p>Important for Data Engineers</p> <p>OLTP \u2192 Source systems</p> <p>ETL/ELT \u2192 Moves data</p> <p>OLAP \u2192 Data warehouse / Analytics system</p> <p>Understanding both is critical for designing data pipelines.</p> <p>Hirarchy in Databricks.</p> <p>Workspace &gt; Catalog &gt; Schema &gt; Tables</p> <p>\ud83d\udd1f Summary</p> <p>Databricks is a modern data platform that:</p> <p>Implements Lakehouse architecture</p> <p>Uses Delta Lake for reliability</p> <p>Supports Medallion architecture for structured data processing</p> <p>Provides centralized governance via Unity Catalog</p> <p>Enables scalable data engineering, analytics, and AI workloads</p> <p>It simplifies big data processing while maintaining enterprise-grade governance and reliability.</p>"},{"location":"DataEngineering/api/","title":"Application Programming Interface","text":""},{"location":"DataEngineering/api/#1-what-is-an-api","title":"1\ufe0f\u20e3 What is an API?","text":"<p>An API (Application Programming Interface) is a set of rules that allows one software application to communicate with another.</p> <p>\ud83d\udc49 In simple terms:</p> <p>API is a messenger that takes your request to a system and brings back the response.</p> <p>Real Life Example</p> <p>You \u2192 Order food</p> <p>Waiter \u2192 API</p> <p>Kitchen \u2192 Server</p> <p>You don\u2019t go inside the kitchen. You talk to the waiter (API).</p> <p>2\ufe0f\u20e3 Why Do We Need APIs?</p> <p>As a Data Analyst / Data Scientist, APIs are important because:</p> <p>Fetch live data from servers</p> <p>Integrate backend with frontend</p> <p>Connect services (Power BI, Tableau, Apps)</p> <p>Automate data pipelines</p> <p>Build ML model serving systems</p>"},{"location":"DataEngineering/api/#3-types-of-apis","title":"3\ufe0f\u20e3 Types of APIs","text":"<ol> <li>Open API (Public API)</li> </ol> <p>Available to everyone</p> <p>Example: Weather API</p> <ol> <li>Internal API</li> </ol> <p>Used within an organization</p> <ol> <li>Partner API</li> </ol> <p>Shared with specific partners</p> <ol> <li>Composite API</li> </ol> <p>Combines multiple APIs in one call</p>"},{"location":"DataEngineering/api/#4-api-architecture-types","title":"4\ufe0f\u20e3 API Architecture Types","text":"<p>\ud83d\udd39 1. REST API (Most Common)</p> <p>Uses HTTP protocol</p> <p>Stateless</p> <p>Uses JSON</p> <p>Very popular in web applications</p> <p>Example:</p> <p>GET https://api.example.com/users</p> <p>\ud83d\udd39 2. SOAP API</p> <p>Uses XML</p> <p>More secure</p> <p>Used in banking systems</p> <p>\ud83d\udd39 3. GraphQL</p> <p>Client requests exactly the data it needs</p> <p>Reduces over-fetching</p> <p>Example:</p> <p>{   user(id: 1) {     name     email   } }</p>"},{"location":"DataEngineering/api/#5-http-methods-very-important","title":"5\ufe0f\u20e3 HTTP Methods (Very Important)","text":"<p>Method  Purpose</p> <p>GET Retrieve data</p> <p>POST    Create data</p> <p>PUT Update data</p> <p>PATCH   Partial update</p> <p>DELETE  Remove data</p>"},{"location":"DataEngineering/api/#6-http-status-codes","title":"6\ufe0f\u20e3 HTTP Status Codes","text":"Code Meaning Description 200 Success The request was successful and the server returned the requested data. 201 Created A new resource was successfully created (usually after POST request). 400 Bad Request The request was invalid or incorrectly formatted. 401 Unauthorized Authentication is required or failed. 403 Forbidden The client does not have permission to access the resource. 404 Not Found The requested resource could not be found. 500 Server Error Internal server error occurred on the server side."},{"location":"DataEngineering/api/#7-api-request-structure","title":"7\ufe0f\u20e3 API Request Structure","text":"<p>Example Request</p> <p>GET /users/1 HTTP/1.1</p> <p>Host: api.example.com</p> <p>Authorization: Bearer  <p>Components of API Request</p> <p>Endpoint (URL)</p> <p>Method (GET, POST)</p> <p>Headers</p> <p>Body (optional)</p>"},{"location":"DataEngineering/api/#8-json-most-used-data-format","title":"8\ufe0f\u20e3 JSON (Most Used Data Format)","text":"<p>Sample JSON Response {   \"id\": 1,   \"name\": \"Akshay\",   \"role\": \"Data Analyst\" }</p> <p>Why JSON?</p> <p>Lightweight</p> <p>Easy to parse</p> <p>Human readable</p> <p>9\ufe0f\u20e3 How API Works (Step-by-Step Flow)</p> <p></p> <p></p> <p></p> <p>Client sends request</p> <p>Server processes request</p> <p>Database interaction</p> <p>Server sends response</p> <p>Client receives data</p>"},{"location":"DataEngineering/api/#api-authentication","title":"\ud83d\udd1f API Authentication","text":"<ol> <li>API Key</li> </ol> <p>Simple key provided in request.</p> <p>?api_key=123456</p> <ol> <li>Bearer Token (JWT)</li> </ol> <p>Authorization: Bearer eyJhbGciOiJIUzI1...</p> <ol> <li>OAuth 2.0</li> </ol> <p>Used for login via:</p> <p>Google</p> <p>Facebook</p> <p>GitHub</p> <p>1\ufe0f\u20e31\ufe0f\u20e3 REST API Example using Python (requests)</p> <p>Install:</p> <p>pip install requests</p> <p>Example:</p> <p>import requests</p> <p>url = \"https://jsonplaceholder.typicode.com/posts/1\"</p> <p>response = requests.get(url)</p> <p>print(\"Status Code:\", response.status_code)</p> <p>print(\"Response JSON:\", response.json())</p>"},{"location":"DataEngineering/api/#12-creating-your-own-api-using-flask","title":"1\ufe0f\u20e32\ufe0f\u20e3 Creating Your Own API using Flask","text":"<p>Install:</p> <p>pip install flask app.py from flask import Flask, jsonify, request</p> <p>app = Flask(name)</p> <p>data = [     {\"id\": 1, \"name\": \"Akshay\"},     {\"id\": 2, \"name\": \"Rahul\"} ]</p> <p>@app.route('/users', methods=['GET']) def get_users():     return jsonify(data)</p> <p>@app.route('/users', methods=['POST']) def add_user():     new_user = request.json     data.append(new_user)     return jsonify({\"message\": \"User added\"}), 201</p> <p>if name == 'main':     app.run(debug=True)</p> <p>Run:</p> <p>python app.py</p> <p>Open:</p> <p>http://127.0.0.1:5000/users</p>"},{"location":"DataEngineering/api/#13-api-testing-tools","title":"1\ufe0f\u20e33\ufe0f\u20e3 API Testing Tools","text":"<p>4 1. Postman</p> <p>Most popular tool for API testing.</p> <ol> <li>Swagger</li> </ol> <p>Auto API documentation.</p> <ol> <li>cURL</li> </ol> <p>Command-line API testing.</p> <p>Example:</p> <p>curl https://jsonplaceholder.typicode.com/posts/1</p> <p>1\ufe0f\u20e34\ufe0f\u20e3 Rate Limiting</p> <p>API limits the number of requests per minute.</p> <p>Example:</p> <p>100 requests per minute</p> <p>If exceeded \u2192 429 Too Many Requests</p> <p>1\ufe0f\u20e35\ufe0f\u20e3 Pagination</p> <p>When API has large data:</p> <p>GET /users?page=1&amp;limit=10</p> <p>1\ufe0f\u20e36\ufe0f\u20e3 API Versioning</p> <p>/api/v1/users</p> <p>/api/v2/users</p> <p>Why?</p> <p>Maintain backward compatibility</p> <p>1\ufe0f\u20e37\ufe0f\u20e3 Best Practices</p> <p>Use HTTPS</p> <p>Proper status codes</p> <p>Use nouns in endpoint</p> <p>Keep it stateless</p> <p>Proper documentation</p> <p>Input validation</p> <p>Error handling</p> <p>1\ufe0f\u20e38\ufe0f\u20e3 API vs Webhook</p> <p>API Webhook</p> <p>Client requests data    Server sends data automatically</p> <p>Pull mechanism  Push mechanism</p> <p>1\ufe0f\u20e39\ufe0f\u20e3 API in Data Science</p> <p>Fetch live stock data</p> <p>Deploy ML model as API</p> <p>Connect Power BI to backend</p> <p>Microservices architecture</p> <p>Automate ETL pipelines</p> <p>2\ufe0f\u20e30\ufe0f\u20e3 Deploying ML Model as API (FastAPI Example)</p> <p>pip install fastapi uvicorn</p> <p>from fastapi import FastAPI</p> <p>import joblib</p> <p>app = FastAPI()</p> <p>model = joblib.load(\"model.pkl\")</p> <p>@app.get(\"/\") def home():     return {\"message\": \"ML API Running\"}</p> <p>@app.post(\"/predict\") def predict(data: dict):     prediction = model.predict([data[\"input\"]])     return {\"prediction\": prediction.tolist()}</p> <p>Run:</p> <p>uvicorn app:app --reload</p>"},{"location":"DataEngineering/de-cycle/","title":"Data Engineering Lifecycle","text":""},{"location":"DataEngineering/de-cycle/#1-data-generation","title":"1. Data Generation","text":"<p>Applications, APIs, logs.</p>"},{"location":"DataEngineering/de-cycle/#2-data-ingestion","title":"2. Data Ingestion","text":"<p>Batch or Streaming.</p>"},{"location":"DataEngineering/de-cycle/#3-storage","title":"3. Storage","text":"<p>Data Lake, Warehouse, Lakehouse.</p>"},{"location":"DataEngineering/de-cycle/#4-processing","title":"4. Processing","text":"<p>Spark, SQL, Python.</p>"},{"location":"DataEngineering/de-cycle/#5-serving","title":"5. Serving","text":"<p>BI tools and ML models.</p>"},{"location":"DataEngineering/de-cycle/#test-data","title":"Test data","text":""},{"location":"DataEngineering/networking/","title":"IP Address &amp; Ports","text":""},{"location":"DataEngineering/networking/#1-what-is-networking","title":"\ud83c\udf0d 1\ufe0f\u20e3 What is Networking?","text":"<p>Networking allows devices to communicate with each other over a network (LAN or Internet).</p> <p>Example:</p> <p>Your laptop</p> <p>A cloud server (AWS/Azure)</p> <p>A database server</p> <p>An API server</p> <p>All communicate using IP addresses and Ports.</p>"},{"location":"DataEngineering/networking/#2-what-is-an-ip-address","title":"\ud83e\udded 2\ufe0f\u20e3 What is an IP Address?","text":"<p>\u2705 Definition</p> <p>An IP (Internet Protocol) Address is a unique identifier assigned to each device on a network.</p> <p>\ud83d\udc49 It works like a home address for your computer.</p> <p>\ud83d\udd39 Example of IP Address</p> <p>192.168.1.10</p> <p>\ud83d\udd39 Types of IP Addresses</p>"},{"location":"DataEngineering/networking/#1-ipv4-most-common","title":"1\ufe0f\u20e3 IPv4 (Most Common)","text":"<p>32-bit number</p> <p>Format: x.x.x.x</p> <p>Range: 0 \u2013 255 per section</p> <p>Example:</p> <p>192.168.0.1</p> <p>2\ufe0f\u20e3 IPv6 (Modern Version)</p> <p>128-bit number</p> <p>Created due to shortage of IPv4</p> <p>Much larger address space</p> <p>Example:</p> <p>2001:0db8:85a3:0000:0000:8a2e:0370:7334</p>"},{"location":"DataEngineering/networking/#3-public-vs-private-ip","title":"\ud83c\udfe0 3\ufe0f\u20e3 Public vs Private IP","text":"<p>Type    Description Example</p> <p>Public IP   Accessible from internet    8.8.8.8</p> <p>Private IP  Used inside local network   192.168.x.x</p>"},{"location":"DataEngineering/networking/#4-what-is-a-port","title":"\ud83d\udd0c 4\ufe0f\u20e3 What is a Port?","text":"<p>\u2705 Definition</p> <p>A Port is a logical communication endpoint on a device.</p> <p>\ud83d\udc49 IP = Building address \ud83d\udc49 Port = Apartment number</p> <p>Without port, server doesn\u2019t know which service to send data to.</p>"},{"location":"DataEngineering/networking/#port-number-range","title":"\ud83d\udd22 Port Number Range","text":"<p>Range   Type</p> <p>0 \u2013 1023    Well-known ports</p> <p>1024 \u2013 49151    Registered ports</p> <p>49152 \u2013 65535   Dynamic/Private ports</p> <p>\ud83d\udd39 Common Port Numbers</p> <p>Port    Service</p> <p>80  HTTP</p> <p>443 HTTPS</p> <p>22  SSH</p> <p>21  FTP</p> <p>3306    MySQL</p> <p>5432    PostgreSQL</p> <p>5000    Flask default</p> <p>8000    FastAPI default</p> <p>6667    KAFKA</p>"},{"location":"DataEngineering/networking/#5-ip-port-together","title":"\ud83c\udf10 5\ufe0f\u20e3 IP + Port Together","text":"<p>When you run Flask:</p> <p>python app.py</p> <p>It runs at:</p> <p>http://127.0.0.1:5000</p> <p>Breakdown:</p> <p>127.0.0.1 \u2192 IP address (localhost)</p> <p>5000 \u2192 Port number</p> <p>\ud83d\udd39 What is 127.0.0.1?</p> <p>It is called:</p> <p>Localhost</p> <p>It means:</p> <p>Your own computer</p> <p>Loopback address</p> <p>Used for testing</p>"},{"location":"DataEngineering/networking/#6-how-communication-works","title":"\ud83d\udd04 6\ufe0f\u20e3 How Communication Works","text":"<p>Step-by-step:</p> <p>Client sends request to IP + Port</p> <p>Router directs traffic</p> <p>Server listens on that port</p> <p>Server processes request</p> <p>Response sent back</p>"},{"location":"DataEngineering/networking/#7-what-does-listening-on-a-port-mean","title":"\ud83e\udde0 7\ufe0f\u20e3 What Does \"Listening on a Port\" Mean?","text":"<p>When you start a server:</p> <p>app.run(port=8000)</p> <p>The server is:</p> <p>Listening for incoming connections on port 8000.</p> <p>If no app listens on a port \u2192 connection fails.</p>"},{"location":"DataEngineering/networking/#8-how-to-check-your-ip","title":"\ud83d\udd0d 8\ufe0f\u20e3 How to Check Your IP","text":"<p>Windows  - ipconfig</p> <p>Mac/Linux - ifconfig</p>"},{"location":"DataEngineering/networking/#9-important-concepts","title":"\ud83d\udd25 9\ufe0f\u20e3 Important Concepts","text":"<p>\ud83d\udd39 Open Port</p> <p>If a port is open:</p> <p>It accepts incoming traffic</p> <p>Example:</p> <p>Port 3306 open \u2192 MySQL accessible</p> <p>\ud83d\udd39 Closed Port</p> <p>No service running \u2192 connection refused.</p> <p>\ud83d\udd39 Firewall</p> <p>Controls which ports are allowed or blocked.</p> <p>Example:</p> <p>Cloud server allows only port 80 &amp; 443</p> <p>\ud83d\udcca 1\ufe0f\u20e30\ufe0f\u20e3 Real Example (API Deployment)</p> <p>Suppose:</p> <p>Server Public IP: 13.234.55.10</p> <p>FastAPI running on port 8000</p> <p>You access:</p> <p>http://13.234.55.10:8000</p> <p>If port 8000 blocked in firewall \u2192 Not accessible.</p> <p>\ud83d\ude80 1\ufe0f\u20e31\ufe0f\u20e3 Networking in Data Science</p> <p>Since you're learning API &amp; deployment, IP and ports are important for:</p> <p>Deploying ML models</p> <p>Hosting dashboards</p> <p>Connecting Power BI to MySQL</p> <p>Cloud VM setup</p> <p>Docker container networking</p>"},{"location":"DataEngineering/networking/#ip-address-classes-ipv4","title":"\ud83c\udf10 IP Address Classes (IPv4)","text":"<p>\ud83d\udccc Definition</p> <p>IP address classes are categories in the original IPv4 addressing system that divide IP addresses into groups based on:</p> <p>Network size</p> <p>Number of hosts</p> <p>First octet (first number) range</p> <p>This system is called Classful Addressing.</p> <p>An IPv4 address is a 32-bit number, written as:</p> <p>x.x.x.x</p> <p>Each octet ranges from 0 to 255.</p> <p>\ud83c\udd70\ufe0f Class A</p> <p>Designed for very large networks</p> <p>Uses 8 bits for network, 24 bits for hosts</p> <p>First Octet Range:</p> <p>1 \u2013 126</p> <p>IP Range:</p> <p>1.0.0.0 \u2013 126.255.255.255</p> <p>Private Range:</p> <p>10.0.0.0 \u2013 10.255.255.255</p> <p>Hosts per Network: ~16 million</p> <p>\ud83c\udd71\ufe0f Class B</p> <p>Designed for medium-sized networks</p> <p>Uses 16 bits for network, 16 bits for hosts</p> <p>First Octet Range:</p> <p>128 \u2013 191</p> <p>IP Range:</p> <p>128.0.0.0 \u2013 191.255.255.255</p> <p>Private Range:</p> <p>172.16.0.0 \u2013 172.31.255.255</p> <p>Hosts per Network: ~65,000</p> <p>\ud83c\udd72 Class C</p> <p>Designed for small networks</p> <p>Uses 24 bits for network, 8 bits for hosts</p> <p>First Octet Range:</p> <p>192 \u2013 223</p> <p>IP Range:</p> <p>192.0.0.0 \u2013 223.255.255.255</p> <p>Private Range:</p> <p>192.168.0.0 \u2013 192.168.255.255</p> <p>Hosts per Network: 254</p> <p>\ud83c\udd73 Class D</p> <p>Used for multicasting</p> <p>Not used for assigning devices</p> <p>First Octet Range:</p> <p>224 \u2013 239</p> <p>IP Range:</p> <p>224.0.0.0 \u2013 239.255.255.255</p> <p>\ud83c\udd74 Class E</p> <p>Used for research and experimental purposes</p> <p>Not used publicly</p> <p>First Octet Range:</p> <p>240 \u2013 255</p> <p>IP Range:</p> <p>240.0.0.0 \u2013 255.255.255.255</p> <p>\ud83d\udccc Quick Summary</p> <p>Concept Meaning</p> <p>IP Address  Identifies device</p> <p>Port    Identifies service on device</p> <p>IP + Port   Identifies specific application</p> <p>127.0.0.1   Local machine</p> <p>Public IP   Internet accessible</p> <p>Private IP  Local network only</p>"},{"location":"DataScience/machine-learning/","title":"ML Algorithms","text":""}]}