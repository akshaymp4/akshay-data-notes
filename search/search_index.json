{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"DataBricks/introduction/","title":"1\ufe0f\u20e3 Introduction","text":"<p>\ud83d\udcd8 What is Databricks?</p> <p>Databricks is a cloud-based unified data and AI platform built on top of Apache Spark that enables organizations to process, analyze, and build machine learning solutions on large-scale data.</p> <p>It is designed around the Lakehouse architecture, which combines:</p> <p>Data Lake capabilities \u2192 Low-cost storage, flexibility, support for structured and unstructured data</p> <p>Data Warehouse capabilities \u2192 High performance, ACID transactions, governance, and BI optimization</p> <p>This means Databricks gives you the flexibility of a data lake and the reliability of a data warehouse in a single platform.</p> <p>\ud83d\ude80 Why Databricks is Called a Managed Service</p> <p>Databricks is a managed service, meaning you do not need to manually set up or maintain infrastructure.</p> <p>Instead of configuring servers, installing Spark, handling failures, and tuning performance \u2014 Databricks manages these for you automatically.</p> <p>It takes care of:</p> <p>Cluster management Infrastructure provisioning Scaling Performance optimization Security integrations</p> <p>This allows:</p> <p>Data Engineers \u2192 to focus on pipelines Analysts \u2192 to focus on insights Data Scientists \u2192 to focus on models Instead of spending time on DevOps or infrastructure setup.</p> <p>1\ufe0f\u20e3 Cluster Management</p> <p>Cluster management refers to the automatic creation, configuration, monitoring, and termination of compute clusters used to process data.</p> <p>\ud83d\udc49 Databricks automatically manages Spark clusters so users don\u2019t need to manually configure servers.</p> <p>2\ufe0f\u20e3 Infrastructure Provisioning</p> <p>Infrastructure provisioning is the process of setting up cloud resources such as virtual machines, storage, and networking.</p> <p>\ud83d\udc49 Databricks automatically provisions the required cloud infrastructure (AWS, Azure, GCP) when you start a cluster.</p> <p>3\ufe0f\u20e3 Scaling</p> <p>Scaling is the ability to increase or decrease computing resources based on workload demand.</p> <p>\ud83d\udc49 Databricks supports auto-scaling, meaning it can add or remove worker nodes automatically depending on workload size.</p> <p>4\ufe0f\u20e3 Performance Optimization</p> <p>Performance optimization involves tuning system resources and execution strategies to run workloads faster and more efficiently.</p> <p>\ud83d\udc49 Databricks optimizes Spark jobs automatically using features like query optimization, caching, and optimized execution engines.</p> <p>5\ufe0f\u20e3 Security Integrations</p> <p>Security integrations ensure that data access and system usage are secure and compliant with organizational policies.</p> <p>\ud83d\udc49 Databricks integrates with cloud IAM systems, role-based access control, encryption, and Unity Catalog for governance.</p> <p>\ud83d\udca1 In Simple Words </p> <p>Without Databricks \u2192 You manage servers, install Spark, scale manually, secure everything yourself.</p> <p>With Databricks \u2192 You just write code. The platform manages everything else</p> <p>This allows data engineers, analysts, and data scientists to focus on building data solutions instead of managing infrastructure.</p>"},{"location":"DataBricks/introduction/#lakehouse-architecture-workflow-diagram","title":"Lakehouse architecture workflow diagram","text":"<p>2\ufe0f\u20e3 Key Components of Databricks</p> <p>Workspaces \u2013 Collaborative environment for notebooks, jobs, and dashboards</p> <p>Clusters \u2013 Compute resources to run Spark workloads</p> <p>DBFS (Databricks File System) \u2013 Distributed file system abstraction</p> <p>Delta Lake \u2013 Storage layer providing ACID transactions</p> <p>Unity Catalog \u2013 Centralized governance layer</p>"},{"location":"DataBricks/introduction/#data-lake-vs-data-warehouse-vs-lakehouse","title":"\ud83d\udd0e Data Lake vs Data Warehouse vs Lakehouse","text":"Feature Data Lake Data Warehouse Lakehouse (Databricks) Storage Cost Low High Low Schema Flexible Structured Structured + Flexible Performance Medium High High ACID Support \u274c No \u2705 Yes \u2705 Yes (Delta Lake) Governance Limited Strong Strong (Unity Catalog) Supports ML \u2705 Yes Limited \u2705 Yes <p>3\ufe0f\u20e3 What is Metadata?</p> <p>Metadata is \u201cdata about data.\u201d</p> <p>It provides information that describes, explains, or gives context to other data.</p> <p>Examples of Metadata:</p> <p>Table name Column names Data types File location Owner Created date Permissions Metadata helps in: Data discovery Governance Access control Query optimization</p> <p>4\ufe0f\u20e3 Managed Tables vs External Tables</p> <p>Databricks supports two main types of tables:</p> <p>\ud83d\udd39 Managed Tables</p> <p>In managed tables, Databricks manages both metadata and physical data storage.</p> <p>Characteristics:</p> <p>Storage location controlled by Databricks Dropping table deletes both metadata and data Strong governance using Unity Catalog Suitable for fully controlled environments</p> <p>Multi-tool Access: Difficult Data Governance: Fully governed Use Case: Quick analytics, internal BI systems, tightly controlled environments</p> <p>\ud83d\udd39 External Tables</p> <p>In external tables, Databricks manages only metadata, while data remains in external storage (like S3, ADLS, GCS).</p> <p>Characteristics:</p> <p>Data stored outside Databricks-managed location Dropping table removes only metadata Flexible integration with other tools Requires governance discipline</p> <p>Multi-tool Access: Easy Data Governance: Flexible but requires discipline Use Case: Shared datasets, existing data lakes, multi-tool ecosystems</p>"},{"location":"DataBricks/introduction/#managed-vs-external-tables","title":"\ud83d\udd0e Managed vs External Tables","text":"Feature Managed Table External Table Metadata Management Databricks Databricks Data Storage Managed by Databricks Stored externally (S3/ADLS/GCS) Data Deletion Metadata + Data deleted Only metadata deleted Multi-tool Access Limited Easy Governance Fully controlled Flexible Best For Internal analytics Shared datasets / Existing data lakes <p>5\ufe0f\u20e3 Lakehouse Architecture</p> <p>Databricks implements the Lakehouse Architecture, which combines:</p> <p>Data Lake   Data Warehouse Cheap storage   High performance Flexible schema Structured governance Raw data storage    BI-ready data</p> <p>Lakehouse provides:</p> <p>ACID transactions Schema enforcement Time travel Batch + Streaming support</p>"},{"location":"DataBricks/introduction/#lakehouse-architecture-overview","title":"\ud83d\udcca Lakehouse Architecture Overview","text":"<p>6\ufe0f\u20e3 \ud83c\udfd7\ufe0f Medallion Architecture (Lakehouse Design Pattern)</p> <p>The Medallion Architecture is a data design pattern used in Databricks to organize data into three layers:</p>"},{"location":"DataBricks/introduction/#medallion-architecture-and-unity-catalog-overview","title":"\ud83c\udfd7\ufe0f Medallion architecture and unity catalog overview","text":"<p>\ud83e\udd49 Bronze Layer \u2013 Raw Data</p> <p>Ingested data from source systems Minimal transformation Used for auditing and traceability</p> <p>\ud83e\udd48 Silver Layer \u2013 Cleaned &amp; Transformed Data</p> <p>Data cleaning Deduplication Standardization Schema enforcement</p> <p>\ud83e\udd47 Gold Layer \u2013 Business-Level Data</p> <p>Aggregated and curated datasets Business KPIs Optimized for reporting and analytics</p> <p>This layered approach improves:</p> <p>Data quality Maintainability Performance Governance</p>"},{"location":"DataBricks/introduction/#bronze-vs-silver-vs-gold","title":"\ud83d\udcca Bronze vs Silver vs Gold","text":"Layer Purpose Data Quality Transformation Level Used By Bronze Raw ingestion Low Minimal Data Engineers Silver Cleaned &amp; standardized Medium Moderate Data Engineers / Analysts Gold Business-ready data High Aggregated &amp; Curated BI / Business Users <p>Source \u2192 Bronze \u2192 Silver \u2192 Gold \u2192 BI / ML</p> <p>7\ufe0f\u20e3 Delta Lake</p> <p>Delta Lake is the storage layer of Databricks that adds reliability to data lakes.</p> <p>It provides:</p> <p>ACID transactions Schema enforcement Schema evolution Time travel (versioning) Scalable metadata handling Delta Lake solves common data lake problems such as: Dirty reads Data corruption Concurrent write issues</p> <p>8\ufe0f\u20e3 Unity Catalog</p> <p>Unity Catalog is Databricks\u2019 unified governance solution for data and AI assets.</p> <p>It manages:</p> <p>Tables Views Files ML models Permissions Lineage tracking</p> <p>Benefits:</p> <p>Centralized access control Fine-grained permissions (row/column level) Data lineage tracking Cross-workspace governance It ensures secure and compliant data usage across the organization.</p> <p>9\ufe0f\u20e3 ACID Principles</p> <p>Databricks (via Delta Lake) supports ACID properties:</p> <p>A \u2013 Atomicity A transaction either fully completes or fully fails.</p> <p>C \u2013 Consistency Data remains valid before and after a transaction.</p> <p>I \u2013 Isolation Concurrent transactions do not interfere with each other.</p> <p>D \u2013 Durability Once committed, data remains stored even if failures occur.</p> <p>ACID ensures reliability in large-scale data systems.</p>"},{"location":"DataBricks/introduction/#acid-properties-in-delta-lake","title":"\ud83d\udd12 ACID Properties in Delta Lake","text":"Property Meaning Example Atomicity All or nothing execution Failed transaction rolls back Consistency Data remains valid Constraints enforced Isolation Transactions do not interfere Concurrent writes handled safely Durability Data remains after commit Data persists after crash <p>\ud83d\udd1f Summary</p> <p>Databricks is a modern data platform that:</p> <p>Implements Lakehouse architecture</p> <p>Uses Delta Lake for reliability</p> <p>Supports Medallion architecture for structured data processing</p> <p>Provides centralized governance via Unity Catalog</p> <p>Enables scalable data engineering, analytics, and AI workloads</p> <p>It simplifies big data processing while maintaining enterprise-grade governance and reliability.</p>"},{"location":"DataEngineering/de-cycle/","title":"Data Engineering Lifecycle","text":""},{"location":"DataEngineering/de-cycle/#1-data-generation","title":"1. Data Generation","text":"<p>Applications, APIs, logs.</p>"},{"location":"DataEngineering/de-cycle/#2-data-ingestion","title":"2. Data Ingestion","text":"<p>Batch or Streaming.</p>"},{"location":"DataEngineering/de-cycle/#3-storage","title":"3. Storage","text":"<p>Data Lake, Warehouse, Lakehouse.</p>"},{"location":"DataEngineering/de-cycle/#4-processing","title":"4. Processing","text":"<p>Spark, SQL, Python.</p>"},{"location":"DataEngineering/de-cycle/#5-serving","title":"5. Serving","text":"<p>BI tools and ML models.</p>"},{"location":"DataEngineering/de-cycle/#test-data","title":"Test data","text":""},{"location":"DataScience/machine-learning/","title":"ML Algorithms","text":""}]}